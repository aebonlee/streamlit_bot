import os
import json
import time
import re
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import streamlit as st
from dataclasses import dataclass

# ===== ë°ì´í„° êµ¬ì¡° =====
@dataclass
class CoverLetterProject:
    job_title: str
    jd_text: str
    resume_text: str
    questions: str
    draft: Optional[str] = None
    refined: Optional[str] = None
    keywords: Optional[List[str]] = None
    coverage: Optional[Dict] = None
    model: str = "gpt-4o-mini"
    temperature: float = 0.7
    target_len: int = 800
    tone: str = "ì •ì¤‘í•˜ê³  ê°„ê²°í•œ"
    timestamp: float = 0.0

# ===== OpenAI API ë˜í¼ =====
class OpenAIClient:
    def __init__(self):
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        try:
            from openai import OpenAI
            api_key = os.getenv("OPENAI_API_KEY") or st.session_state.get("api_key", "")
            if api_key:
                self.client = OpenAI(api_key=api_key)
                self.is_v1 = True
            else:
                raise Exception("No API key found")
        except Exception:
            try:
                import openai
                api_key = os.getenv("OPENAI_API_KEY") or st.session_state.get("api_key", "")
                if api_key:
                    openai.api_key = api_key
                    self.client = openai
                    self.is_v1 = False
                else:
                    raise Exception("No API key found")
            except Exception as e:
                st.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.client = None
    
    def call_openai(self, model: str, messages: List[Dict], temperature: float = 0.7, max_tokens: int = 1200) -> str:
        if not self.client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            if self.is_v1:
                resp = self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                return resp.choices[0].message.content
            else:
                resp = self.client.ChatCompletion.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                return resp["choices"][0]["message"]["content"]
        except Exception as e:
            if "rate limit" in str(e).lower():
                raise Exception("API í˜¸ì¶œ í•œë„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            elif "invalid api key" in str(e).lower():
                raise Exception("ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ì…ë‹ˆë‹¤.")
            else:
                raise Exception(f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

# ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =====
class TextAnalyzer:
    @staticmethod
    def count_korean_chars(text: str) -> int:
        """í•œê¸€ ë¬¸ì ìˆ˜ ê³„ì‚°"""
        return len(re.sub(r'[^\uAC00-\uD7A3]', '', text))
    
    @staticmethod
    def analyze_readability(text: str) -> Dict:
        """ê°€ë…ì„± ë¶„ì„"""
        sentences = re.split(r'[.!?]\s*', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return {"avg_sentence_length": 0, "long_sentences": 0, "readability_score": 0}
        
        sentence_lengths = [len(s) for s in sentences]
        avg_length = sum(sentence_lengths) / len(sentence_lengths)
        long_sentences = sum(1 for length in sentence_lengths if length > 100)
        
        # ê°„ë‹¨í•œ ê°€ë…ì„± ì ìˆ˜ (100ì  ë§Œì )
        readability_score = max(0, min(100, 100 - (avg_length - 50) * 2 - long_sentences * 10))
        
        return {
            "avg_sentence_length": round(avg_length, 1),
            "long_sentences": long_sentences,
            "readability_score": round(readability_score, 1),
            "total_sentences": len(sentences)
        }
    
    @staticmethod
    def detect_cliche_advanced(text: str) -> Dict:
        """ê³ ë„í™”ëœ í´ë¦¬ì…° ê°ì§€"""
        cliche_patterns = {
            "ê³¼ë„í•œ í˜•ìš©ì‚¬": ["ë§¤ìš°", "ì •ë§", "ë„ˆë¬´", "êµ‰ì¥íˆ", "ì—„ì²­", "ìµœê³ ì˜", "ì™„ë²½í•œ"],
            "ë»”í•œ í‘œí˜„": ["ì—´ì •ì ìœ¼ë¡œ", "ëŠì„ì—†ì´ ë…¸ë ¥", "í•­ìƒ ìµœì„ ", "ë„ì „ì •ì‹ ", "ì±…ì„ê°", "ì†Œí†µëŠ¥ë ¥"],
            "ì¶”ìƒì  í‘œí˜„": ["ì‹œë„ˆì§€", "ìœˆìœˆ", "íŒŒë¼ë‹¤ì„", "ë²¤ì¹˜ë§ˆí‚¹", "ê¸€ë¡œë²Œ ë§ˆì¸ë“œ"],
            "ê³¼ì¥ í‘œí˜„": ["í˜ì‹ ì ì¸", "ì°¨ë³„í™”ëœ", "ë…ì°½ì ì¸", "íƒì›”í•œ", "ë›°ì–´ë‚œ"]
        }
        
        detected = {}
        total_cliche = 0
        
        for category, words in cliche_patterns.items():
            found = []
            for word in words:
                count = text.count(word)
                if count > 0:
                    found.append(f"{word}({count})")
                    total_cliche += count
            detected[category] = found
        
        return {
            "categories": detected,
            "total_cliche_count": total_cliche,
            "cliche_density": round(total_cliche / max(1, len(text)) * 1000, 2)  # 1000ìë‹¹ í´ë¦¬ì…° ìˆ˜
        }

class KeywordAnalyzer:
    def __init__(self, openai_client: OpenAIClient):
        self.client = openai_client
    
    def extract_keywords(self, text: str, top_k: int = 20) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        if not text.strip():
            return []
        
        prompt = f"""
        ì•„ë˜ ì±„ìš© ê³µê³ /ì§ë¬´ ì„¤ëª…ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
        
        ì¶”ì¶œ ê¸°ì¤€:
        1. ì§ë¬´ ê´€ë ¨ ì „ë¬¸ ìš©ì–´ ë° ê¸°ìˆ 
        2. í•„ìˆ˜ ì—­ëŸ‰ ë° ìê²©ìš”ê±´
        3. ìš°ëŒ€ì‚¬í•­
        4. íšŒì‚¬/ì¡°ì§ ë¬¸í™” ê´€ë ¨ í‚¤ì›Œë“œ
        
        í˜•ì‹: ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ìµœëŒ€ {top_k}ê°œ
        
        í…ìŠ¤íŠ¸:
        {text}
        """
        
        try:
            response = self.client.call_openai("gpt-4o-mini", [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ], temperature=0.1, max_tokens=400)
            
            keywords = [kw.strip().strip(".,Â·") for kw in response.split(",") if kw.strip()]
            return keywords[:top_k]
        except Exception as e:
            st.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_coverage(self, draft: str, keywords: List[str]) -> Dict:
        """í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ (ê°œì„ ëœ ë²„ì „)"""
        if not draft or not keywords:
            return {"covered": [], "missing": keywords, "coverage": 0.0, "partial_matches": []}
        
        draft_lower = draft.lower()
        covered = []
        partial_matches = []
        missing = []
        
        for keyword in keywords:
            kw_lower = keyword.lower()
            if kw_lower in draft_lower:
                covered.append(keyword)
            elif any(word in draft_lower for word in kw_lower.split()):
                partial_matches.append(keyword)
            else:
                missing.append(keyword)
        
        coverage = round(100.0 * len(covered) / max(1, len(keywords)), 1)
        partial_coverage = round(100.0 * len(partial_matches) / max(1, len(keywords)), 1)
        
        return {
            "covered": covered,
            "partial_matches": partial_matches,
            "missing": missing,
            "coverage": coverage,
            "partial_coverage": partial_coverage,
            "total_coverage": coverage + partial_coverage * 0.5
        }

class CoverLetterGenerator:
    def __init__(self, openai_client: OpenAIClient):
        self.client = openai_client
    
    def generate_draft(self, project: CoverLetterProject) -> str:
        """ì´ˆì•ˆ ìƒì„±"""
        context = self._build_context(project)
        
        prompt = f"""
        ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ ìê¸°ì†Œê°œì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        {context}
        
        ì‘ì„± ê°€ì´ë“œë¼ì¸:
        1. ê° ë¬¸í•­ë³„ë¡œ êµ¬ì²´ì ì´ê³  ì°¨ë³„í™”ëœ ë‹µë³€ ì‘ì„±
        2. STAR ê¸°ë²• í™œìš© (Situation, Task, Action, Result)
        3. ì •ëŸ‰ì  ì„±ê³¼ì™€ êµ¬ì²´ì  ì‚¬ë¡€ í¬í•¨
        4. {project.tone} í†¤ ìœ ì§€
        5. ë¬¸í•­ ì œëª©ì„ **êµµê²Œ** í‘œì‹œ
        6. ì±„ìš©ê³µê³ ì˜ í•µì‹¬ í‚¤ì›Œë“œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
        
        ê¸ˆì§€ì‚¬í•­:
        - ì¼ë°˜ì ì´ê³  ì¶”ìƒì ì¸ í‘œí˜„ ë‚¨ë°œ
        - ê·¼ê±° ì—†ëŠ” ê³¼ì¥
        - íƒ€ì¸ì˜ ì„±ê³¼ ë„ìš©
        """
        
        try:
            response = self.client.call_openai(project.model, [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ìê¸°ì†Œê°œì„œ ì‘ì„± ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ], temperature=project.temperature, max_tokens=1500)
            
            return response
        except Exception as e:
            raise Exception(f"ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def refine_text(self, text: str, project: CoverLetterProject) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ ë° ê¸¸ì´ ì¡°ì •"""
        current_chars = TextAnalyzer.count_korean_chars(text)
        
        prompt = f"""
        ë‹¤ìŒ ìê¸°ì†Œê°œì„œë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”.
        
        í˜„ì¬ ê¸¸ì´: {current_chars}ì
        ëª©í‘œ ê¸¸ì´: {project.target_len}ì (Â±50ì)
        í†¤: {project.tone}
        
        ê°œì„  ì‚¬í•­:
        1. ëª©í‘œ ê¸¸ì´ì— ë§ê²Œ ì¡°ì •
        2. ë¬¸ì¥ êµ¬ì¡° ê°œì„  ë° ê°€ë…ì„± í–¥ìƒ
        3. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°
        4. êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ ì„±ê³¼ëŠ” ë³´ì¡´
        5. ë…¼ë¦¬ì  íë¦„ ê°•í™”
        
        ì›ë¬¸:
        {text}
        """
        
        try:
            response = self.client.call_openai(project.model, [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ì—ë””í„°ì…ë‹ˆë‹¤. ì •í™•í•œ ê¸¸ì´ ì¡°ì •ê³¼ í’ˆì§ˆ í–¥ìƒì„ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ], temperature=0.4, max_tokens=1200)
            
            return response
        except Exception as e:
            raise Exception(f"í…ìŠ¤íŠ¸ ì •ì œ ì‹¤íŒ¨: {e}")
    
    def _build_context(self, project: CoverLetterProject) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        if project.job_title:
            context_parts.append(f"ì§€ì› ì§ë¬´/íšŒì‚¬: {project.job_title}")
        
        if project.jd_text:
            context_parts.append(f"ì±„ìš© ê³µê³ /ì§ë¬´ ê¸°ìˆ ì„œ:\n{project.jd_text}")
        
        if project.resume_text:
            context_parts.append(f"ì§€ì›ì ì´ë ¥/ê²½í—˜:\n{project.resume_text}")
        
        if project.questions:
            context_parts.append(f"ìê¸°ì†Œê°œì„œ ë¬¸í•­:\n{project.questions}")
        
        return "\n\n".join(context_parts)

# ===== Streamlit ì•± =====
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    defaults = {
        "project": CoverLetterProject("", "", "", ""),
        "openai_client": None,
        "keyword_analyzer": None,
        "generator": None,
        "api_key": ""
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

def setup_sidebar():
    """ì‚¬ì´ë“œë°” ì„¤ì •"""
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # API í‚¤ ì„¤ì •
        if not os.getenv("OPENAI_API_KEY"):
            api_key = st.text_input("OpenAI API Key", type="password", value=st.session_state.api_key)
            if api_key != st.session_state.api_key:
                st.session_state.api_key = api_key
                st.session_state.openai_client = None  # í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” í•„ìš”
        
        # ëª¨ë¸ ì„¤ì •
        project = st.session_state.project
        project.model = st.selectbox("ëª¨ë¸", ["gpt-4o-mini", "gpt-4o", "gpt-4o-2024-08-06"], index=0)
        project.temperature = st.slider("ì°½ì˜ì„± (temperature)", 0.0, 1.2, 0.7, 0.1)
        project.target_len = st.number_input("ëª©í‘œ ê¸€ììˆ˜(í•œê¸€)", 200, 3000, 800, 50)
        project.tone = st.selectbox("í†¤/ìŠ¤íƒ€ì¼", [
            "ì •ì¤‘í•˜ê³  ê°„ê²°í•œ", "ì—´ì •ì ì´ê³  ì§ì„¤ì ì¸", "ë…¼ë¦¬ì ì´ê³  ë¶„ì„ì ì¸", 
            "ë”°ëœ»í•˜ê³  ìŠ¤í† ë¦¬í…”ë§ ìœ„ì£¼", "ì°½ì˜ì ì´ê³  ë„ì „ì ì¸"
        ])
        
        # ê³ ê¸‰ ì˜µì…˜
        with st.expander("ğŸ”§ ê³ ê¸‰ ì˜µì…˜"):
            show_analysis = st.checkbox("ìƒì„¸ ë¶„ì„ í‘œì‹œ", True)
            auto_save = st.checkbox("ìë™ ì €ì¥", False)
            
        return show_analysis, auto_save

def initialize_clients():
    """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    if st.session_state.openai_client is None:
        try:
            st.session_state.openai_client = OpenAIClient()
            st.session_state.keyword_analyzer = KeywordAnalyzer(st.session_state.openai_client)
            st.session_state.generator = CoverLetterGenerator(st.session_state.openai_client)
        except Exception as e:
            st.error(f"í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    return True

def render_input_form():
    """ì…ë ¥ í¼ ë Œë”ë§"""
    st.subheader("ğŸ“ 1) ê¸°ë³¸ ì •ë³´ ì…ë ¥")
    
    project = st.session_state.project
    
    col1, col2 = st.columns(2)
    with col1:
        project.job_title = st.text_input(
            "ì§€ì› ì§ë¬´/íšŒì‚¬", 
            value=project.job_title,
            placeholder="ì˜ˆ: ë°ì´í„° ë¶„ì„ê°€ - ì‚¼ì„±ì „ì"
        )
        
        project.jd_text = st.text_area(
            "ì±„ìš© ê³µê³ /ì§ë¬´ ê¸°ìˆ ì„œ", 
            value=project.jd_text,
            height=200,
            placeholder="ì±„ìš©ê³µê³ ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”. ìš”êµ¬ì—­ëŸ‰, ìš°ëŒ€ì‚¬í•­ ë“±ì´ í¬í•¨ë˜ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤."
        )
    
    with col2:
        project.resume_text = st.text_area(
            "ì´ë ¥/ê²½í—˜ ìš”ì•½", 
            value=project.resume_text,
            height=200,
            placeholder="ì£¼ìš” í”„ë¡œì íŠ¸, ì„±ê³¼(ì •ëŸ‰ì  ìˆ˜ì¹˜), ë³´ìœ  ê¸°ìˆ  ë“±ì„ ê°„ëµíˆ ì •ë¦¬í•´ì£¼ì„¸ìš”."
        )
    
    project.questions = st.text_area(
        "ìê¸°ì†Œê°œì„œ ë¬¸í•­ (í•œ ì¤„ì— í•˜ë‚˜ì”©)",
        value=project.questions or ("ì§€ì› ë™ê¸°ì™€ ì…ì‚¬ í›„ í¬ë¶€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
                                   "ë³¸ì¸ì˜ ê°•ì ê³¼ ì´ë¥¼ ì¦ëª…í•˜ëŠ” ì‚¬ë¡€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
                                   "í˜‘ì—… ê²½í—˜ê³¼ ê°ˆë“± í•´ê²° ì‚¬ë¡€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."),
        height=120
    )
    
    # ì…ë ¥ ê²€ì¦
    missing_fields = []
    if not project.job_title.strip():
        missing_fields.append("ì§€ì› ì§ë¬´/íšŒì‚¬")
    if not project.questions.strip():
        missing_fields.append("ìê¸°ì†Œê°œì„œ ë¬¸í•­")
    
    if missing_fields:
        st.warning(f"í•„ìˆ˜ ì…ë ¥ ì‚¬í•­: {', '.join(missing_fields)}")
        return False
    
    return True

def render_generation_buttons():
    """ìƒì„± ë²„íŠ¼ ë Œë”ë§"""
    st.subheader("ğŸš€ 2) ìƒì„±")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        generate_draft = st.button("ğŸ“„ ì´ˆì•ˆ ìƒì„±", use_container_width=True)
    with col2:
        refine_text = st.button("âœ¨ ê¸¸ì´/í†¤ ë³´ì •", use_container_width=True, 
                               disabled=not st.session_state.project.draft)
    with col3:
        generate_all = st.button("ğŸ¯ ì „ì²´ ìƒì„±", use_container_width=True)
    with col4:
        analyze_keywords = st.button("ğŸ” í‚¤ì›Œë“œ ë¶„ì„", use_container_width=True)
    
    return generate_draft, refine_text, generate_all, analyze_keywords

def process_generation(generate_draft, refine_text, generate_all, analyze_keywords):
    """ìƒì„± í”„ë¡œì„¸ìŠ¤ ì²˜ë¦¬"""
    project = st.session_state.project
    
    if not initialize_clients():
        return
    
    # ì´ˆì•ˆ ìƒì„±
    if generate_draft or generate_all:
        with st.spinner("ğŸ“ ì´ˆì•ˆì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                project.draft = st.session_state.generator.generate_draft(project)
                project.timestamp = time.time()
                st.success("âœ… ì´ˆì•ˆ ìƒì„± ì™„ë£Œ!")
            except Exception as e:
                st.error(f"âŒ ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
                return
    
    # ê¸¸ì´/í†¤ ë³´ì •
    if (refine_text or generate_all) and project.draft:
        with st.spinner("âœ¨ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                project.refined = st.session_state.generator.refine_text(project.draft, project)
                st.success("âœ… ë³´ì • ì™„ë£Œ!")
            except Exception as e:
                st.error(f"âŒ ë³´ì • ì‹¤íŒ¨: {e}")
    
    # í‚¤ì›Œë“œ ë¶„ì„
    if (analyze_keywords or generate_all) and project.jd_text:
        with st.spinner("ğŸ” í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                project.keywords = st.session_state.keyword_analyzer.extract_keywords(project.jd_text)
                
                if project.refined or project.draft:
                    text_to_analyze = project.refined or project.draft
                    project.coverage = st.session_state.keyword_analyzer.analyze_coverage(
                        text_to_analyze, project.keywords
                    )
                
                st.success("âœ… í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ!")
            except Exception as e:
                st.error(f"âŒ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")

def render_results():
    """ê²°ê³¼ ë Œë”ë§"""
    st.subheader("ğŸ“‹ 3) ê²°ê³¼")
    
    project = st.session_state.project
    
    # íƒ­ìœ¼ë¡œ êµ¬ì„±
    tab1, tab2, tab3 = st.tabs(["ğŸ“„ ì´ˆì•ˆ", "âœ¨ ë³´ì •ë³¸", "ğŸ“Š ë¹„êµ"])
    
    with tab1:
        if project.draft:
            chars = TextAnalyzer.count_korean_chars(project.draft)
            st.metric("ê¸€ì ìˆ˜", f"{chars:,}ì")
            st.markdown(project.draft)
        else:
            st.info("ì•„ì§ ì´ˆì•ˆì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    with tab2:
        if project.refined:
            chars = TextAnalyzer.count_korean_chars(project.refined)
            target_diff = chars - project.target_len
            st.metric("ê¸€ì ìˆ˜", f"{chars:,}ì", f"{target_diff:+}ì (ëª©í‘œ ëŒ€ë¹„)")
            st.markdown(project.refined)
        else:
            st.info("ì•„ì§ ë³´ì •ë³¸ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    with tab3:
        if project.draft and project.refined:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ì´ˆì•ˆ")
                draft_chars = TextAnalyzer.count_korean_chars(project.draft)
                st.metric("ê¸€ì ìˆ˜", f"{draft_chars:,}ì")
                st.text_area("", project.draft, height=300, disabled=True)
            
            with col2:
                st.subheader("ë³´ì •ë³¸")
                refined_chars = TextAnalyzer.count_korean_chars(project.refined)
                char_diff = refined_chars - draft_chars
                st.metric("ê¸€ì ìˆ˜", f"{refined_chars:,}ì", f"{char_diff:+}ì")
                st.text_area("", project.refined, height=300, disabled=True)
        else:
            st.info("ë¹„êµí•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

def render_analysis(show_analysis):
    """ë¶„ì„ ê²°ê³¼ ë Œë”ë§"""
    if not show_analysis:
        return
    
    project = st.session_state.project
    text_to_analyze = project.refined or project.draft
    
    if not text_to_analyze:
        return
    
    st.subheader("ğŸ“Š 4) ìƒì„¸ ë¶„ì„")
    
    # í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€
    if project.coverage and project.keywords:
        st.subheader("ğŸ¯ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì™„ì „ ë§¤ì¹­", f"{project.coverage['coverage']:.1f}%")
        with col2:
            st.metric("ë¶€ë¶„ ë§¤ì¹­", f"{project.coverage['partial_coverage']:.1f}%")
        with col3:
            st.metric("ì¢…í•© ì ìˆ˜", f"{project.coverage['total_coverage']:.1f}%")
        
        # í‚¤ì›Œë“œ ìƒì„¸
        if project.coverage['covered']:
            st.success("âœ… í¬í•¨ëœ í‚¤ì›Œë“œ: " + ", ".join(project.coverage['covered']))
        
        if project.coverage['partial_matches']:
            st.warning("ğŸ”¶ ë¶€ë¶„ ë§¤ì¹­: " + ", ".join(project.coverage['partial_matches']))
        
        if project.coverage['missing']:
            st.error("âŒ ëˆ„ë½ëœ í‚¤ì›Œë“œ: " + ", ".join(project.coverage['missing']))
    
    # ê°€ë…ì„± ë¶„ì„
    st.subheader("ğŸ“– ê°€ë…ì„± ë¶„ì„")
    readability = TextAnalyzer.analyze_readability(text_to_analyze)
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{readability['avg_sentence_length']:.1f}ì")
    with col2:
        st.metric("ê¸´ ë¬¸ì¥ ìˆ˜", f"{readability['long_sentences']}ê°œ")
    with col3:
        st.metric("ì´ ë¬¸ì¥ ìˆ˜", f"{readability['total_sentences']}ê°œ")
    with col4:
        st.metric("ê°€ë…ì„± ì ìˆ˜", f"{readability['readability_score']:.1f}")
    
    # í´ë¦¬ì…° ë¶„ì„
    st.subheader("ğŸ­ í´ë¦¬ì…° ë¶„ì„")
    cliche_analysis = TextAnalyzer.detect_cliche_advanced(text_to_analyze)
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("í´ë¦¬ì…° ë°€ë„", f"{cliche_analysis['cliche_density']:.2f}")
        st.caption("1000ìë‹¹ í´ë¦¬ì…° ê°œìˆ˜")
    with col2:
        st.metric("ì´ í´ë¦¬ì…° ìˆ˜", f"{cliche_analysis['total_cliche_count']}ê°œ")
    
    # í´ë¦¬ì…° ìƒì„¸
    for category, items in cliche_analysis['categories'].items():
        if items:
            st.warning(f"**{category}**: {', '.join(items)}")

def render_export():
    """ë‚´ë³´ë‚´ê¸° ê¸°ëŠ¥"""
    st.subheader("ğŸ’¾ 5) ë‚´ë³´ë‚´ê¸°")
    
    project = st.session_state.project
    export_text = project.refined or project.draft
    
    if not export_text:
        st.info("ë‚´ë³´ë‚¼ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Markdown ë‹¤ìš´ë¡œë“œ
        md_content = f"""# ìê¸°ì†Œê°œì„œ

**ì§€ì› ì§ë¬´**: {project.job_title}
**ì‘ì„±ì¼**: {datetime.fromtimestamp(project.timestamp).strftime('%Y-%m-%d %H:%M:%S') if project.timestamp else ''}

{export_text}

---
*AI ìê¸°ì†Œê°œì„œ ì‘ì„±ê¸°ë¡œ ìƒì„±ë¨*
"""
        st.download_button(
            "ğŸ“„ Markdown ë‹¤ìš´ë¡œë“œ", 
            data=md_content.encode("utf-8"), 
            file_name=f"cover_letter_{int(time.time())}.md", 
            mime="text/markdown",
            use_container_width=True
        )
    
    with col2:
        # JSON í”„ë¡œì íŠ¸ íŒŒì¼
        project_data = {
            "job_title": project.job_title,
            "jd_text": project.jd_text,
            "resume_text": project.resume_text,
            "questions": project.questions,
            "draft": project.draft,
            "refined": project.refined,
            "keywords": project.keywords,
            "coverage": project.coverage,
            "model": project.model,
            "temperature": project.temperature,
            "target_len": project.target_len,
            "tone": project.tone,
            "timestamp": project.timestamp,
            "version": "2.0"
        }
        
        st.download_button(
            "ğŸ’¾ í”„ë¡œì íŠ¸ ì €ì¥", 
            data=json.dumps(project_data, ensure_ascii=False, indent=2),
            file_name=f"cover_letter_project_{int(time.time())}.json", 
            mime="application/json",
            use_container_width=True
        )
    
    with col3:
        # í…ìŠ¤íŠ¸ íŒŒì¼
        st.download_button(
            "ğŸ“ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ", 
            data=export_text.encode("utf-8"), 
            file_name=f"cover_letter_{int(time.time())}.txt", 
            mime="text/plain",
            use_container_width=True
        )

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="âœï¸ AI ìê¸°ì†Œê°œì„œ ì‘ì„±ê¸° Pro", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # í—¤ë”
    st.title("âœï¸ AI ìê¸°ì†Œê°œì„œ ì‘ì„±ê¸° Pro")
    st.caption("ğŸ“ ì…ë ¥ â†’ ğŸš€ ì´ˆì•ˆ ìƒì„± â†’ âœ¨ ê¸¸ì´/í†¤ ë³´ì • â†’ ğŸ” í‚¤ì›Œë“œ ë¶„ì„ â†’ ğŸ“Š í’ˆì§ˆ í‰ê°€ â†’ ğŸ’¾ ë‚´ë³´ë‚´ê¸°")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    show_analysis, auto_save = setup_sidebar()
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if render_input_form():
        # ìƒì„± ë²„íŠ¼
        generate_draft, refine_text, generate_all, analyze_keywords = render_generation_buttons()
        
        # ìƒì„± í”„ë¡œì„¸ìŠ¤
        process_generation(generate_draft, refine_text, generate_all, analyze_keywords)
        
        # ê²°ê³¼ í‘œì‹œ
        render_results()
        
        # ë¶„ì„ ê²°ê³¼
        render_analysis(show_analysis)
        
        # ë‚´ë³´ë‚´ê¸°
        render_export()
    
    # í‘¸í„°
    st.markdown("---")
    st.caption("ğŸ’¡ **íŒ**: êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì„±ê³¼ë¥¼ í¬í•¨í•˜ë©´ ë” ì¢‹ì€ ìê¸°ì†Œê°œì„œê°€ ì™„ì„±ë©ë‹ˆë‹¤!")

if __name__ == "__main__":
    main()